要求低时间延迟的数据访问的应用不适合运行在HDFS上，例如几十毫秒的范围。
目前对于低延迟的访问需求，HBase是更好的选择。

每个文件大约需要150字节的存储空间，那么100万个文件，占多少空间呢？300M？

HDFS中的文件不支持有多个写入者的操作，也不支持在文件的任意位置进行修改。

HDFS也有block的概念，默认是64M。HDFS中小于一个块大小的文件不会占据整个块的空间。
64M的大的块定义，为了最小化寻址的开销。这样，时间的消耗主要在数据传输上。很多情况下，HDFS使用128M大小的块定义。
块也不能设置的过大，因为MapReduce中的Map任务通常一次处理一个块中的数据，如果任务数太少的话，处理速度就会变慢。

HBASE的安装比较难。
